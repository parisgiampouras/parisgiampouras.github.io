<!DOCTYPE html> <html lang="en"> <head> <meta name="google-site-verification" content="glSZ0hQ3w9nTmh1xyPXYcJdIjWb7Uct6sddwO1h-N8g"/> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>publications | Paris Giampouras</title> <meta name="author" content="Paris Giampouras"/> <meta name="description" content="Publications at peer-reviewed journals and conferences"/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <meta property="og:site_name" content="Paris Giampouras"/> <meta property="og:type" content="website"/> <meta property="og:title" content="Paris Giampouras | publications"/> <meta property="og:url" content="https://parisgiampouras.github.io/publications/"/> <meta property="og:description" content="Publications at peer-reviewed journals and conferences"/> <meta property="og:locale" content="en"/> <meta name="twitter:card" content="summary"/> <meta name="twitter:title" content="publications"/> <meta name="twitter:description" content="Publications at peer-reviewed journals and conferences"/> <meta name="twitter:site" content="@parisgiampouras"/> <meta name="twitter:creator" content="@parisgiampouras"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⚛️</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://parisgiampouras.github.io/publications/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Paris </span>Giampouras</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/repsai/">RepsAI Lab</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">publications</h1> <p class="post-description">Publications at peer-reviewed journals and conferences</p> </header> <article> <div class="publications"> <h2 class="year">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ICML</abbr></div> <div id="giampouras2024guarantees" class="col-sm-8"> <div class="title">Guarantees of a preconditioned subgradient algorithm for overparameterized asymmetric low-rank matrix recovery</div> <div class="author"> <em>Paris Giampouras</em>, HanQin Cai, and René Vidal</div> <div class="periodical"> <em>International Conference on Machine Learning (ICML)</em> 2025 </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">giampouras2024guarantees</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Guarantees of a preconditioned subgradient algorithm for overparameterized asymmetric low-rank matrix recovery}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Giampouras, Paris and Cai, HanQin and Vidal, Ren{\'e}}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{International Conference on Machine Learning (ICML)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ICML</abbr></div> <div id="mildner2025federated" class="col-sm-8"> <div class="title">Federated Generalised Variational Inference: A Robust Probabilistic Federated Learning Framework</div> <div class="author"> Terje Mildner, Oliver Hamelijnck, <em>Paris Giampouras</em>, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Theodoros Damoulas' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>International Conference on Machine Learning (ICML) (spotlight)</em> 2025 </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">mildner2025federated</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Federated Generalised Variational Inference: A Robust Probabilistic Federated Learning Framework}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mildner, Terje and Hamelijnck, Oliver and Giampouras, Paris and Damoulas, Theodoros}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{International Conference on Machine Learning (ICML) (spotlight)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="year">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">Signal Processing</abbr></div> <div id="rontogiannis2023online" class="col-sm-8"> <div class="title">Online rank-revealing block-term tensor decomposition</div> <div class="author"> Athanasios A Rontogiannis, Eleftherios Kofidis, and Paris V Giampouras</div> <div class="periodical"> <em>Signal Processing</em> 2023 </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">rontogiannis2023online</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Online rank-revealing block-term tensor decomposition}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Rontogiannis, Athanasios A and Kofidis, Eleftherios and Giampouras, Paris V}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Signal Processing}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{212}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{109126}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Elsevier}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ICCV Workshop</abbr></div> <div id="lamers2023clustering" class="col-sm-8"> <div class="title">Clustering-based domain-incremental learning</div> <div class="author"> Christiaan Lamers, René Vidal, Nabil Belbachir, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Niki Stein, Thomas Bäeck, Paris Giampouras' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> 2023 </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">lamers2023clustering</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Clustering-based domain-incremental learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lamers, Christiaan and Vidal, Ren{\'e} and Belbachir, Nabil and van Stein, Niki and B{\"a}eck, Thomas and Giampouras, Paris}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Proceedings of the IEEE/CVF International Conference on Computer Vision}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{3384--3392}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">Arxiv</abbr></div> <div id="thaker2023linearly" class="col-sm-8"> <div class="title">A Linearly Convergent GAN Inversion-based Algorithm for Reverse Engineering of Deceptions</div> <div class="author"> Darshan Thaker, <em>Paris Giampouras</em>, and René Vidal</div> <div class="periodical"> <em>arXiv preprint arXiv:2306.04756</em> 2023 </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">thaker2023linearly</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Linearly Convergent GAN Inversion-based Algorithm for Reverse Engineering of Deceptions}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Thaker, Darshan and Giampouras, Paris and Vidal, Ren{\'e}}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2306.04756}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ICML</abbr></div> <div id="peng2023ideal" class="col-sm-8"> <div class="title">The ideal continual learner: An agent that never forgets</div> <div class="author"> Liangzu Peng, <em>Paris Giampouras</em>, and René Vidal</div> <div class="periodical"> 2023 </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">peng2023ideal</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{The ideal continual learner: An agent that never forgets}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Peng, Liangzu and Giampouras, Paris and Vidal, Ren{\'e}}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{International Conference on Machine Learning}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{27585--27610}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{PMLR}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="year">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">EUSIPCO</abbr></div> <div id="9909799" class="col-sm-8"> <div class="title">A Projected Newton-type Algorithm for Rank - revealing Nonnegative Block - Term Tensor Decomposition</div> <div class="author"> Eleftherios Kofidis, <em>Paris Giampouras</em>, and Athanasios A. Rontogiannis</div> <div class="periodical"> <em>30th European Signal Processing Conference (EUSIPCO)</em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>The block-term tensor decomposition (BTD) model has been receiving increasing attention as a quite flexible way to capture the structure of 3-dimensional data that can be naturally viewed as the superposition of R block terms of multilinear rank ( Lr,Lr,1),r=1,2,…,R . Versions with nonnegativity constraints, especially relevant in applications like blind source separation problems, have only recently been proposed and they all share the need to have an a-priori knowledge of the number of block terms, R , and their individual ranks, Li . Clearly, the latter requirement may severely limit their practical applicability. Building upon earlier work of ours on unconstrained BTD model selection and computation, we develop for the first time in this paper a method for nonnegative BTD approximation that is also rank-revealing. The idea is to impose column sparsity jointly on the factors and successively estimate the ranks as the numbers of factor columns of non-negligible magnitude. This is effected with the aid of nonnegative alternating iteratively reweighted least squares, implemented via projected Newton updates for increased convergence rate and accuracy. Simulation results are reported that demonstrate the effectiveness of our method in accurately estimating both the ranks and the factors of the nonnegative least squares BTD approximation.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">9909799</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kofidis, Eleftherios and Giampouras, Paris and Rontogiannis, Athanasios A.}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{30th European Signal Processing Conference (EUSIPCO)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Projected Newton-type Algorithm for Rank - revealing Nonnegative Block - Term Tensor Decomposition}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1961-1965}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ICLR</abbr></div> <div id="giampouras2022implicit" class="col-sm-8"> <div class="title">Implicit Bias of Projected Subgradient Method Gives Provable Robust Recovery of Subspaces of Unknown Codimension</div> <div class="author"> <em>Paris Giampouras</em>, Benjamin Haeffele, and Rene Vidal</div> <div class="periodical"> <em>International Conference on Learning Representations</em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Robust subspace recovery (RSR) is the problem of learning a subspace from sample data points corrupted by outliers. Dual Principal Component Pursuit (DPCP) is a robust subspace recovery method that aims to find a basis for the orthogonal complement of the subspace by minimizing the sum of the distances of the points to the subspaces subject to orthogonality constraints on the basis. Prior work has shown that DPCP can provably recover the correct subspace in the presence of outliers as long as the true dimension of the subspace is known. In this paper, we show that if the orthogonality constraints –adopted in previous DPCP formulations– are relaxed and random initialization is used instead of spectral one, DPCP can provably recover a subspace of \emphunknown dimension. Specifically, we propose a very simple algorithm based on running multiple instances of a projected sub-gradient descent method (PSGM), with each problem instance seeking to find one vector in the null space of the subspace. We theoretically prove that under mild conditions this approach succeeds with high probability. In particular, we show that 1) all of the problem instances will converge to a vector in the nullspace of the subspace and 2) the ensemble of problem instance solutions will be sufficiently diverse to fully span the nullspace of the subspace thus also revealing its true unknown codimension. We provide empirical results that corroborate our theoretical results and showcase the remarkable implicit rank regularization behavior of the PSGM algorithm that allows us to perform RSR without knowing the subspace dimension</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">giampouras2022implicit</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Implicit Bias of Projected Subgradient Method Gives Provable Robust Recovery of Subspaces of Unknown Codimension}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Giampouras, Paris and Haeffele, Benjamin and Vidal, Rene}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{International Conference on Learning Representations}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=vA7doMdgi75}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">IEEE TSP</abbr></div> <div id="9735405" class="col-sm-8"> <div class="title">Block-Term Tensor Decomposition Model Selection and Computation: The Bayesian Way</div> <div class="author"> <em>Paris Giampouras</em>, Athanasios A. Rontogiannis, and Eleftherios Kofidis</div> <div class="periodical"> <em>IEEE Transactions on Signal Processing</em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>The so-called block-term decomposition (BTD) tensor model, especially in its rank- (Lr,Lr,1) version, has been recently receiving increasing attention due to its enhanced ability of representing systems and signals that are composed of blocks of rank higher than one, a scenario encountered in numerous and diverse applications. Uniqueness conditions and fitting methods have thus been thoroughly studied. Nevertheless, the challenging problem of estimating the BTD model structure, namely the number of block terms, R , and their individual ranks, Lr , has only recently started to attract significant attention, mainly through regularization-based approaches which entail the need to tune the regularization parameter(s). In this work, we build on ideas of sparse Bayesian learning (SBL) and put forward a fully automated Bayesian approach. Through a suitably crafted multi-level hierarchical probabilistic model, which gives rise to heavy-tailed prior distributions for the BTD factors, structured sparsity is jointly imposed. Ranks are then estimated from the numbers of blocks ( R ) and columns ( Lr ) of non-negligible energy. Approximate posterior inference is implemented, within the variational inference framework. The resulting iterative algorithm completely avoids hyperparameter tuning, which is a significant defect of regularization-based methods. Alternative probabilistic models are also explored and the connections with their regularization-based counterparts are brought to light with the aid of the associated maximum a-posteriori (MAP) estimators. We report simulation results with both synthetic and real-word data, which demonstrate the merits of the proposed method in terms of both rank estimation and model fitting as compared to state-of-the-art relevant methods.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">9735405</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Giampouras, Paris and Rontogiannis, Athanasios A. and Kofidis, Eleftherios}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Signal Processing}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Block-Term Tensor Decomposition Model Selection and Computation: The Bayesian Way}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{70}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1704-1717}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/TSP.2022.3159029}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ICML</abbr></div> <div id="pmlr-v162-thaker22a" class="col-sm-8"> <div class="title">Reverse Engineering \ell_p attacks: A block-sparse optimization approach with recovery guarantees</div> <div class="author"> Darshan Thaker *, Paris Giampouras *, and Rene Vidal</div> <div class="periodical"> <em>(* equal contribution) 39th International Conference on Machine Learning</em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://proceedings.mlr.press/v162/thaker22a/thaker22a.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p>Deep neural network-based classifiers have been shown to be vulnerable to imperceptible perturbations to their input, such as \ell_p-bounded norm adversarial attacks. This has motivated the development of many defense methods, which are then broken by new attacks, and so on. This paper focuses on a different but related problem of reverse engineering adversarial attacks. Specifically, given an attacked signal, we study conditions under which one can determine the type of attack (\ell_1, \ell_2 or \ell_∞) and recover the clean signal. We pose this problem as a block-sparse recovery problem, where both the signal and the attack are assumed to lie in a union of subspaces that includes one subspace per class and one subspace per attack type. We derive geometric conditions on the subspaces under which any attacked signal can be decomposed as the sum of a clean signal plus an attack. In addition, by determining the subspaces that contain the signal and the attack, we can also classify the signal and determine the attack type. Experiments on digit and face classification demonstrate the effectiveness of the proposed approach.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">pmlr-v162-thaker22a</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Reverse Engineering $\ell_p$ attacks: A block-sparse optimization approach with recovery guarantees}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Thaker *, Darshan and Giampouras *, Paris and Vidal, Rene}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{(* equal contribution) 39th International Conference on Machine Learning}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{21253--21271}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{162}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{Proceedings of Machine Learning Research}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{PMLR}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="year">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ICASSP</abbr></div> <div id="9415104" class="col-sm-8"> <div class="title">Rank-Revealing Block-Term Decomposition for Tensor Completion</div> <div class="author"> Athanasios A. Rontogiannis, Paris V Giampouras, and Eleftherios Kofidis</div> <div class="periodical"> 2021 </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@articles</span><span class="p">{</span><span class="nl">9415104</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Rontogiannis, Athanasios A. and Giampouras, Paris V and Kofidis, Eleftherios}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Rank-Revealing Block-Term Decomposition for Tensor Completion}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2915-2919}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICASSP39728.2021.9415104}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ASILOMAR</abbr></div> <div id="rontogiannis2021online" class="col-sm-8"> <div class="title">Online rank-revealing block-term tensor decomposition</div> <div class="author"> Athanasios A Rontogiannis, Eleftherios Kofidis, and <em>Paris Giampouras</em> </div> <div class="periodical"> <em>55th Asilomar Conference on Signals, Systems, and Computers</em> 2021 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>The so-called block-term decomposition (BTD) tensor model, especially in its rank-(L r , L r , 1) version, has been recently receiving increasing attention due to its enhanced ability of representing systems and signals that are composed of block components of rank higher than one, a scenario encountered in numerous and diverse applications. Its uniqueness and approximation have thus been thoroughly studied. The challenging problem of estimating the BTD model structure, namely the number of block terms (rank) and their individual (block) ranks, is of crucial importance in practice and has only recently started to attract significant attention. In data-streaming scenarios and/or big data applications, where the tensor dimension in one of its modes grows in time or can only be processed incrementally, it is essential to be able to perform model selection and computation in a recursive (incremental/online) manner. To date there is only one such work in the literature concerning the (general rank-(L, M, N)) BTD model, which proposes an incremental method, however with the BTD rank and block ranks assumed to be a-priori known and time invariant. In this paper, a novel approach to rank-(L r , L r , 1) BTD model selection and tracking is proposed, based on the idea of imposing column sparsity jointly on the factors and estimating the ranks as the numbers of factor columns of nonnegligible magnitude. An online method of the alternating reweighted least squares (RLS) type is developed and shown to be computationally efficient and fast converging, also allowing the model ranks to change in time. Its time and memory efficiency are evaluated and favorably compared with those of the batch approach. Simulation results are reported that demonstrate the effectiveness of the proposed scheme in both selecting and tracking the correct BTD model.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">rontogiannis2021online</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Online rank-revealing block-term tensor decomposition}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Rontogiannis, Athanasios A and Kofidis, Eleftherios and Giampouras, Paris}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{55th Asilomar Conference on Signals, Systems, and Computers}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1678--1682}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">IEEE JSTSP</abbr></div> <div id="9321500" class="col-sm-8"> <div class="title">Block-Term Tensor Decomposition: Model Selection and Computation</div> <div class="author"> Athanasios A. Rontogiannis, Eleftherios Kofidis, and <em>Paris Giampouras</em> </div> <div class="periodical"> <em>IEEE Journal of Selected Topics in Signal Processing</em> 2021 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2002.09759.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p>The so-called block-term decomposition (BTD) tensor model has been recently receiving increasing attention due to its enhanced ability of representing systems and signals that are composed of blocks of rank higher than one, a scenario encountered in numerous and diverse applications. Its uniqueness and approximation have thus been thoroughly studied. Nevertheless, the challenging problem of estimating the BTD model structure, namely the number of block terms and their individual ranks, has only recently started to attract significant attention. In this paper, a novel method of BTD model selection and computation is proposed, based on the idea of imposing column sparsity jointly on the factors and in a hierarchical manner and estimating the ranks as the numbers of factor columns of non-negligible magnitude. Following a block successive upper bound minimization (BSUM) approach for the proposed optimization problem is shown to result in an alternating hierarchical iteratively reweighted least squares (HIRLS) algorithm, which is fast converging and enjoys high computational efficiency, as it relies in its iterations on small-sized sub-problems with closed-form solutions. Simulation results for both synthetic examples and a hyper-spectral image denoising application are reported, which demonstrate the superiority of the proposed scheme over the state-of-the-art in terms of success rate in rank estimation as well as computation time and rate of convergence while attaining a comparable tensor approximation performance.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">9321500</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Rontogiannis, Athanasios A. and Kofidis, Eleftherios and Giampouras, Paris}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Journal of Selected Topics in Signal Processing}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Block-Term Tensor Decomposition: Model Selection and Computation}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{15}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{3}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{464-475}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/JSTSP.2021.3051488}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="year">2020</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">NeurIPS</abbr></div> <div id="NEURIPS2020_f53eb412" class="col-sm-8"> <div class="title">A novel variational form of the Schatten-p quasi-norm</div> <div class="author"> <em>Paris Giampouras</em>, Rene Vidal, Athanasios Rontogiannis, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Benjamin Haeffele' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> 2020 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>The Schatten-p quasi-norm with p ∈ (0, 1) has recently gained considerable atten- tion in various low-rank matrix estimation problems offering significant benefits over relevant convex heuristics such as the nuclear norm. However, due to the nonconvexity of the Schatten-p quasi-norm, minimization suffers from two major drawbacks: 1) the lack of theoretical guarantees and 2) the high computational cost which is demanded for the minimization task even for trivial tasks such as finding stationary points. In an attempt to reduce the high computational cost induced by Schatten-p quasi-norm minimization, variational forms, which are defined over smaller-size matrix factors whose product equals the original matrix, have been proposed. Here, we propose and analyze a novel variational form of Schatten-p quasi-norm which, for the first time in the literature, is defined for any continuous value of p ∈ (0, 1] and decouples along the columns of the factorized matrices. The proposed form can be considered as the natural generalization of the well-known variational form of the nuclear norm to the nonconvex case i.e., for p ∈ (0, 1). Notably, low-rankness is now imposed via a group-sparsity promoting regularizer. The resulting formulation gives way to SVD-free algorithms thus offering lower computational complexity than the one that is induced by the original definition of the Schatten-p quasi-norm. A local optimality analysis is provided which shows that we can arrive at a local minimum of the original Schatten-p quasi-norm problem by reaching a local minimum of the matrix factorization based surrogate problem. In addition, for the case of the squared Frobenius loss with linear operators obeying the restricted isometry property (RIP), a rank-one update scheme is proposed, which offers a way to escape poor local minima. Finally, the efficiency of our approach is empirically shown on a matrix completion problem. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@articles</span><span class="p">{</span><span class="nl">NEURIPS2020_f53eb412</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Giampouras, Paris and Vidal, Rene and Rontogiannis, Athanasios and Haeffele, Benjamin}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Advances in Neural Information Processing Systems}</span><span class="p">,</span>
  <span class="err">%editor</span> <span class="err">=</span> <span class="err">{H.</span> <span class="err">Larochelle</span> <span class="err">and</span> <span class="err">M.</span> <span class="err">Ranzato</span> <span class="err">and</span> <span class="err">R.</span> <span class="err">Hadsell</span> <span class="err">and</span> <span class="err">M.F.</span> <span class="err">Balcan</span> <span class="err">and</span> <span class="err">H.</span> <span class="err">Lin</span><span class="p">}</span><span class="c">,</span>
  <span class="c">%pages = {21453--21463},</span>
  <span class="c">%publisher = {Curran Associates, Inc.},</span>
  <span class="c">title = {A novel variational form of the Schatten-p quasi-norm},</span>
  <span class="c">volume = {33},</span>
  <span class="c">year = {2020}</span>
<span class="c">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">IEEE SP</abbr></div> <div id="9149633" class="col-sm-8"> <div class="title">Online Reweighted Least Squares Robust PCA</div> <div class="author"> Athanasios A. Rontogiannis, <em>Paris Giampouras</em>, and Konstantinos D. Koutroumbas</div> <div class="periodical"> <em>IEEE Signal Processing Letters</em> 2020 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>The letter deals with the problem known as robust principal component analysis (RPCA), that is, the decomposition of a data matrix as the sum of a low-rank matrix component and a sparse matrix component. After expressing the low-rank matrix component in factorized form, we develop a novel online RPCA algorithm that is based entirely on reweighted least squares recursions and is appropriate for sequential data processing. The proposed algorithm is fast, memory optimal and, as corroborated by indicative empirical results on simulated data and a video processing application, competitive to the state-of-the-art in terms of estimation performance.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">9149633</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Rontogiannis, Athanasios A. and Giampouras, Paris and Koutroumbas, Konstantinos D.}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Signal Processing Letters}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Online Reweighted Least Squares Robust PCA}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{27}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1340-1344}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/LSP.2020.3011896}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="year">2019</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ICASSP</abbr></div> <div id="8682910" class="col-sm-8"> <div class="title">A Projected Newton-type Algorithm for Nonnegative Matrix Factorization with Model Order Selection</div> <div class="author"> Paris V. Giampouras, Athanasios A. Rontogiannis, and Konstantinos D. Koutroumbas</div> <div class="periodical"> 2019 </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">8682910</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Giampouras, Paris V. and Rontogiannis, Athanasios A. and Koutroumbas, Konstantinos D.}</span><span class="p">,</span>
  <span class="na">article</span> <span class="p">=</span> <span class="s">{ IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Projected Newton-type Algorithm for Nonnegative Matrix Factorization with Model Order Selection}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICASSP.2019.8682910}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">IEEE TSP</abbr></div> <div id="8552430" class="col-sm-8"> <div class="title">Alternating Iteratively Reweighted Least Squares Minimization for Low-Rank Matrix Factorization</div> <div class="author"> <em>Paris Giampouras</em>, Athanasios A. Rontogiannis, and Konstantinos D. Koutroumbas</div> <div class="periodical"> <em>IEEE Transactions on Signal Processing</em> 2019 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Nowadays, the availability of large-scale data in disparate application domains urges the deployment of sophisticated tools for extracting valuable knowledge out of this huge bulk of information. In that vein, low-rank representations (LRRs), which seek low-dimensional embeddings of data have naturally appeared. In an effort to reduce computational complexity and improve estimation performance, LRR has been viewed via a matrix factorization (MF) perspective. Recently, low-rank MF (LRMF) approaches have been proposed for tackling the inherent weakness of MF, i.e., the unawareness of the dimension of the low-dimensional space where data reside. Herein, inspired by the merits of iterative reweighted schemes for sparse recovery and rank minimization, we come up with a generic low-rank promoting regularization function. Then, focusing on a specific instance of it, we propose a regularizer that imposes column-sparsity jointly on the two matrix factors that result from MF, thus promoting low-rankness on the optimization problem. The low-rank promoting properties of the resulting regularization term are brought to light by mathematically showing that it is actually a tight upper bound of a specific version of the weighted nuclear norm. The problems of denoising and matrix completion are redefined according to the new LRMF formulation and solved via efficient alternating iteratively reweighted least squares type algorithms. Theoretical analysis of the algorithms regarding the convergence and the rates of convergence to stationary points is provided. The effectiveness of the proposed algorithms is verified in diverse simulated and real data experiments.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">8552430</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Giampouras, Paris and Rontogiannis, Athanasios A. and Koutroumbas, Konstantinos D.}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Signal Processing}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Alternating Iteratively Reweighted Least Squares Minimization for Low-Rank Matrix Factorization}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{67}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{2}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{490-503}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/TSP.2018.2883921}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="year">2018</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">IEEE SP </abbr></div> <div id="8401927" class="col-sm-8"> <div class="title">A Computationally Efficient Tensor Completion Algorithm</div> <div class="author"> Ioannis C. Tsaknakis, Paris V. Giampouras, Athanasios A. Rontogiannis, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Konstantinos D. Koutroumbas' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>IEEE Signal Processing Letters</em> 2018 </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">8401927</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tsaknakis, Ioannis C. and Giampouras, Paris V. and Rontogiannis, Athanasios A. and Koutroumbas, Konstantinos D.}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Signal Processing Letters}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Computationally Efficient Tensor Completion Algorithm}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/LSP.2018.2852490}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="year">2017</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">Elsevier SP</abbr></div> <div id="GIAMPOURAS2017199" class="col-sm-8"> <div class="title">Online sparse and low-rank subspace learning from incomplete data: A Bayesian view</div> <div class="author"> <em>Paris Giampouras</em>, Athanasios A. Rontogiannis, Konstantinos E. Themelis, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Konstantinos D. Koutroumbas' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>Signal Processing</em> 2017 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Extracting the underlying low-dimensional space where high-dimensional signals often reside has been at the center of numerous algorithms in the signal processing and machine learning literature during the past few decades. Moreover, working with incomplete large scale datasets has recently been commonplace for diverse reasons. This so called big data era we are currently living calls for devising online subspace learning algorithms that can suitably handle incomplete data. Their anticipated goal is to recursively estimate the unknown subspace by processing streaming data sequentially, thus reducing computational complexity. In this paper, an online variational Bayes subspace learning algorithm from partial observations is presented. To account for the unawareness of the true rank of the subspace, commonly met in practice, low-rankness is explicitly imposed on the sought subspace data matrix by exploiting sparse Bayesian learning principles. Sparsity, simultaneously to low-rankness, is favored on the subspace matrix by the sophisticated hierarchical Bayesian scheme that is adopted. The proposed algorithm is thus adept in dealing with applications whereby the underlying subspace may be also sparse. The new subspace tracking scheme outperforms its state-of-the-art counterparts in terms of estimation accuracy, in a variety of experiments conducted on both simulated and real data.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">GIAMPOURAS2017199</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Online sparse and low-rank subspace learning from incomplete data: A Bayesian view}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Signal Processing}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{137}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{199-212}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{0165-1684}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1016/j.sigpro.2017.02.003}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.sciencedirect.com/science/article/pii/S016516841730049X}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Giampouras, Paris and Rontogiannis, Athanasios A. and Themelis, Konstantinos E. and Koutroumbas, Konstantinos D.}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Subspace tracking, Online matrix completion, Online variational Bayes, Incomplete data, Sparse subspace learning, Low-rank}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="year">2016</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">IEEE TGRS</abbr></div> <div id="7460933" class="col-sm-8"> <div class="title">Simultaneously Sparse and Low-Rank Abundance Matrix Estimation for Hyperspectral Image Unmixing</div> <div class="author"> <em>Paris Giampouras</em>, Konstantinos E. Themelis, Athanasios A. Rontogiannis, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Konstantinos D. Koutroumbas' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>IEEE Transactions on Geoscience and Remote Sensing</em> 2016 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>In a plethora of applications dealing with inverse problems, e.g., image processing, social networks, compressive sensing, and biological data processing, the signal of interest is known to be structured in several ways at the same time. This premise has recently guided research into the innovative and meaningful idea of imposing multiple constraints on the unknown parameters involved in the problem under study. For instance, when dealing with problems whose unknown parameters form sparse and low-rank matrices, the adoption of suitably combined constraints imposing sparsity and low rankness is expected to yield substantially enhanced estimation results. In this paper, we address the spectral unmixing problem in hyperspectral images. Specifically, two novel unmixing algorithms are introduced in an attempt to exploit both spatial correlation and sparse representation of pixels lying in the homogeneous regions of hyperspectral images. To this end, a novel mixed penalty term is first defined consisting of the sum of the weighted ℓ 1 and the weighted nuclear norm of the abundance matrix corresponding to a small area of the image determined by a sliding square window. This penalty term is then used to regularize a conventional quadratic cost function and impose simultaneous sparsity and low rankness on the abundance matrix. The resulting regularized cost function is minimized by: 1) an incremental proximal sparse and low-rank unmixing algorithm; and 2) an algorithm based on the alternating direction method of multipliers. The effectiveness of the proposed algorithms is illustrated in experiments conducted both on simulated and real data.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">7460933</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Simultaneously Sparse and Low-Rank Abundance Matrix Estimation for Hyperspectral Image Unmixing}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Giampouras, Paris and Themelis, Konstantinos E. and Rontogiannis, Athanasios A. and Koutroumbas, Konstantinos D.}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Geoscience and Remote Sensing}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2016}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{54}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{8}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{4775-4789}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/TGRS.2016.2551327}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="year">2015</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">CoSeRa</abbr></div> <div id="7330280" class="col-sm-8"> <div class="title">A sparse reduced-rank regression approach for hyperspectral image unmixing</div> <div class="author"> Paris V. Giampouras, Athanasios A. Rontogiannis, Konstantinos D. Koutroumbas, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Konstantinos E. Themelis' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> 2015 </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">7330280</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Giampouras, Paris V. and Rontogiannis, Athanasios A. and Koutroumbas, Konstantinos D. and Themelis, Konstantinos E.}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{3rd International Workshop on Compressed Sensing Theory and its Applications to Radar, Sonar and Remote Sensing (CoSeRa)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A sparse reduced-rank regression approach for hyperspectral image unmixing}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2015}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/CoSeRa.2015.7330280}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="year">2014</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">IEEE WHISPERS</abbr></div> <div id="8077560" class="col-sm-8"> <div class="title">A variational Bayes algorithm for joint-sparse abundance estimation</div> <div class="author"> <em>Paris Giampouras</em>, Konstantinos E. Themelis, Athanasios A. Rontogiannis, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Konstantinos D. Koutroumbas' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> 2014 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>This paper presents a variational Bayesian scheme for semi-supervised unmixing on hyperspectral images that exploits the inherent spatial correlation between neighboring pixels. More specifically, a hierarchical Bayesian model that promotes a joint-sparse profile on the abundance vectors of adjacent pixels is developed and a computationally efficient variational Bayes algorithm is incorporated to perform Bayesian inference. The benefits of the proposed joint-sparse model are demonstrated via simulations on both synthetic and real data.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@articles</span><span class="p">{</span><span class="nl">8077560</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Giampouras, Paris and Themelis, Konstantinos E. and Rontogiannis, Athanasios A. and Koutroumbas, Konstantinos D.}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{6th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A variational Bayes algorithm for joint-sparse abundance estimation}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2014}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1-4}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/WHISPERS.2014.8077560}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Paris Giampouras. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. <a href="https://parisgiampouras.github.io/contact/">Impressum</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-WZ13PPY09N"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-WZ13PPY09N");</script> </body> </html>