<!DOCTYPE html> <html lang="en"> <head> <meta name="google-site-verification" content="glSZ0hQ3w9nTmh1xyPXYcJdIjWb7Uct6sddwO1h-N8g"/> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Paris Giampouras</title> <meta name="author" content="Paris Giampouras"/> <meta name="description" content="My personal website. "/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <meta property="og:site_name" content="Paris Giampouras"/> <meta property="og:type" content="website"/> <meta property="og:title" content="Paris Giampouras | about"/> <meta property="og:url" content="https://parisgiampouras.github.io/"/> <meta property="og:description" content="My personal website. "/> <meta property="og:locale" content="en"/> <meta name="twitter:card" content="summary"/> <meta name="twitter:title" content="about"/> <meta name="twitter:description" content="My personal website. "/> <meta name="twitter:site" content="@parisgiampouras"/> <meta name="twitter:creator" content="@parisgiampouras"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⚛️</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://parisgiampouras.github.io/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/repsai/">RepsAI Lab</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Paris</span> Giampouras </h1> <p class="desc"><a href="https://www.dcs.warwick.ac.uk/" target="_blank" rel="noopener noreferrer">Assistant Professor, Department of Computer Science, University of Warwick</a><br>Research Scientist (part-time) at <a href="https://ai.meta.com/research/" target="_blank" rel="noopener noreferrer"> FAIR, Meta AI</a></p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/prof_pic-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/prof_pic-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/prof_pic-1400.webp"></source> <img src="/assets/img/prof_pic.jpg" class="img-fluid z-depth-1 rounded-circle" width="auto" height="auto" alt="prof_pic.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="address"> <p>MB2.30, Mathematical Sciences Building</p> <p>University of Warwick,UK </p> </div> </div> <div class="clearfix"> <p>I am an Assistant Professor of Machine Learning and AI in the <a href="https://warwick.ac.uk/fac/sci/dcs/" target="_blank" rel="noopener noreferrer">Department of Computer Science at the University of Warwick</a>, within the Foundations of AI and Machine Learning (<a href="https://warwick.ac.uk/fac/sci/dcs/research/fam/" target="_blank" rel="noopener noreferrer">FAM</a>) division. I am also a part-time Research Scientist at <a href="https://ai.meta.com/research/" target="_blank" rel="noopener noreferrer">FAIR, Meta AI</a> and an Affiliated Researcher at the <a href="https://archimedesai.gr/en/" target="_blank" rel="noopener noreferrer">Archimedes AI Research Center</a>.</p> <p>Previously, I was a Research Faculty member at the <a href="https://www.minds.jhu.edu" target="_blank" rel="noopener noreferrer">Mathematical Institute for Data Science (MINDS) at Johns Hopkins University</a>, where I held a Marie Skłodowska-Curie Postdoctoral Fellowship from 2019 to 2022.</p> <p>My research sits at the intersection of <strong>optimization, probabilistic inference, and representation learning</strong>, with the goal of building <strong>robust, efficient, and adaptive learning systems</strong>. Recent work includes adaptive optimizers for training foundation models, robust federated learning via generalized variational inference, and theory/algorithms for overparameterized recovery and robust tensor methods. I also work on generative modeling and post-training, including steering diffusion and flow-based models via representational alignment for inverse problems, as well as adaptation/alignment of LLMs—particularly in agentic and continual-learning settings.</p> <p><strong>I am a co-organizer of the <a href="https://realm-gen-workshop.github.io/" target="_blank" rel="noopener noreferrer">ReALM–GEN workshop at ICLR 2026</a>, which features an exciting lineup of invited speakers and panelists. Full details are available on the <a href="https://realm-gen-workshop.github.io/" target="_blank" rel="noopener noreferrer">workshop website</a>, and submissions are handled via <a href="https://openreview.net/group?id=ICLR.cc/2026/Workshop/ReALM-GEN" target="_blank" rel="noopener noreferrer">OpenReview</a>.</strong></p> <p><strong>I co-organize the <a href="https://faiseminarswarwick.github.io" target="_blank" rel="noopener noreferrer">Foundations of AI Seminar (FAIS)</a> series. If you’d like to be considered as a future speaker, please <a href="mailto:paris.giampouras@warwick.ac.uk">reach out</a>!</strong></p> <p><strong>Prospective collaborators and students interested in working with me are warmly encouraged to <a href="mailto:paris.giampouras@warwick.ac.uk">e-mail</a> me!</strong></p> </div> <div class="news"> <h2>news</h2> <div class="table-responsive" style="max-height: 10vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row">Jan 11, 2026</th> <td> I am excited to be serving as an Area Chair for <a href="https://icml.cc/Conferences/2026" target="_blank" rel="noopener noreferrer">ICML 2026</a>! </td> </tr> <tr> <th scope="row">Aug 18, 2025</th> <td> I am happy to be serving as an Area Chair for <a href="https://iclr.cc" target="_blank" rel="noopener noreferrer">ICLR 2026</a>! </td> </tr> <tr> <th scope="row">May 30, 2025</th> <td> Two papers (1 <a href="https://arxiv.org/pdf/2410.16826" target="_blank" rel="noopener noreferrer">poster</a> &amp; 1 <a href="https://arxiv.org/pdf/2502.00846" target="_blank" rel="noopener noreferrer">spotlight</a>) were accepted at ICML 2025! See you in Vancouver :)! </td> </tr> <tr> <th scope="row">Oct 10, 2024</th> <td> I am excited to be serving as an Area Chair for both ICLR 2025 and AAMAS 2025! </td> </tr> <tr> <th scope="row">Dec 10, 2023</th> <td> I have joined the Department of the University of Warwick as an Assistant Professor of ML/AI! Currently, I am seeking a talented student to collaborate with for a fully funded PhD position. Please feel free to contact me if you are interested! </td> </tr> </table> </div> </div> <div class="publications"> <h2>selected publications</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ICML</abbr></div> <div id="giampouras2024guarantees" class="col-sm-8"> <div class="title">Guarantees of a preconditioned subgradient algorithm for overparameterized asymmetric low-rank matrix recovery</div> <div class="author"> <em>Paris Giampouras</em>, HanQin Cai, and René Vidal</div> <div class="periodical"> <em>International Conference on Machine Learning (ICML)</em> 2025 </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">giampouras2024guarantees</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Guarantees of a preconditioned subgradient algorithm for overparameterized asymmetric low-rank matrix recovery}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Giampouras, Paris and Cai, HanQin and Vidal, Ren{\'e}}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{International Conference on Machine Learning (ICML)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ICML</abbr></div> <div id="mildner2025federated" class="col-sm-8"> <div class="title">Federated Generalised Variational Inference: A Robust Probabilistic Federated Learning Framework</div> <div class="author"> Terje Mildner, Oliver Hamelijnck, <em>Paris Giampouras</em>, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Theodoros Damoulas' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>International Conference on Machine Learning (ICML) (spotlight)</em> 2025 </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">mildner2025federated</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Federated Generalised Variational Inference: A Robust Probabilistic Federated Learning Framework}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mildner, Terje and Hamelijnck, Oliver and Giampouras, Paris and Damoulas, Theodoros}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{International Conference on Machine Learning (ICML) (spotlight)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ICML</abbr></div> <div id="peng2023ideal" class="col-sm-8"> <div class="title">The ideal continual learner: An agent that never forgets</div> <div class="author"> Liangzu Peng, <em>Paris Giampouras</em>, and René Vidal</div> <div class="periodical"> 2023 </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">peng2023ideal</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{The ideal continual learner: An agent that never forgets}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Peng, Liangzu and Giampouras, Paris and Vidal, Ren{\'e}}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{International Conference on Machine Learning}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{27585--27610}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{PMLR}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ICLR</abbr></div> <div id="giampouras2022implicit" class="col-sm-8"> <div class="title">Implicit Bias of Projected Subgradient Method Gives Provable Robust Recovery of Subspaces of Unknown Codimension</div> <div class="author"> <em>Paris Giampouras</em>, Benjamin Haeffele, and Rene Vidal</div> <div class="periodical"> <em>International Conference on Learning Representations</em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Robust subspace recovery (RSR) is the problem of learning a subspace from sample data points corrupted by outliers. Dual Principal Component Pursuit (DPCP) is a robust subspace recovery method that aims to find a basis for the orthogonal complement of the subspace by minimizing the sum of the distances of the points to the subspaces subject to orthogonality constraints on the basis. Prior work has shown that DPCP can provably recover the correct subspace in the presence of outliers as long as the true dimension of the subspace is known. In this paper, we show that if the orthogonality constraints –adopted in previous DPCP formulations– are relaxed and random initialization is used instead of spectral one, DPCP can provably recover a subspace of \emphunknown dimension. Specifically, we propose a very simple algorithm based on running multiple instances of a projected sub-gradient descent method (PSGM), with each problem instance seeking to find one vector in the null space of the subspace. We theoretically prove that under mild conditions this approach succeeds with high probability. In particular, we show that 1) all of the problem instances will converge to a vector in the nullspace of the subspace and 2) the ensemble of problem instance solutions will be sufficiently diverse to fully span the nullspace of the subspace thus also revealing its true unknown codimension. We provide empirical results that corroborate our theoretical results and showcase the remarkable implicit rank regularization behavior of the PSGM algorithm that allows us to perform RSR without knowing the subspace dimension</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">giampouras2022implicit</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Implicit Bias of Projected Subgradient Method Gives Provable Robust Recovery of Subspaces of Unknown Codimension}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Giampouras, Paris and Haeffele, Benjamin and Vidal, Rene}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{International Conference on Learning Representations}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=vA7doMdgi75}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">IEEE TSP</abbr></div> <div id="9735405" class="col-sm-8"> <div class="title">Block-Term Tensor Decomposition Model Selection and Computation: The Bayesian Way</div> <div class="author"> <em>Paris Giampouras</em>, Athanasios A. Rontogiannis, and Eleftherios Kofidis</div> <div class="periodical"> <em>IEEE Transactions on Signal Processing</em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>The so-called block-term decomposition (BTD) tensor model, especially in its rank- (Lr,Lr,1) version, has been recently receiving increasing attention due to its enhanced ability of representing systems and signals that are composed of blocks of rank higher than one, a scenario encountered in numerous and diverse applications. Uniqueness conditions and fitting methods have thus been thoroughly studied. Nevertheless, the challenging problem of estimating the BTD model structure, namely the number of block terms, R , and their individual ranks, Lr , has only recently started to attract significant attention, mainly through regularization-based approaches which entail the need to tune the regularization parameter(s). In this work, we build on ideas of sparse Bayesian learning (SBL) and put forward a fully automated Bayesian approach. Through a suitably crafted multi-level hierarchical probabilistic model, which gives rise to heavy-tailed prior distributions for the BTD factors, structured sparsity is jointly imposed. Ranks are then estimated from the numbers of blocks ( R ) and columns ( Lr ) of non-negligible energy. Approximate posterior inference is implemented, within the variational inference framework. The resulting iterative algorithm completely avoids hyperparameter tuning, which is a significant defect of regularization-based methods. Alternative probabilistic models are also explored and the connections with their regularization-based counterparts are brought to light with the aid of the associated maximum a-posteriori (MAP) estimators. We report simulation results with both synthetic and real-word data, which demonstrate the merits of the proposed method in terms of both rank estimation and model fitting as compared to state-of-the-art relevant methods.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">9735405</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Giampouras, Paris and Rontogiannis, Athanasios A. and Kofidis, Eleftherios}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Signal Processing}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Block-Term Tensor Decomposition Model Selection and Computation: The Bayesian Way}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{70}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1704-1717}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/TSP.2022.3159029}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ICML</abbr></div> <div id="pmlr-v162-thaker22a" class="col-sm-8"> <div class="title">Reverse Engineering \ell_p attacks: A block-sparse optimization approach with recovery guarantees</div> <div class="author"> Darshan Thaker *, Paris Giampouras *, and Rene Vidal</div> <div class="periodical"> <em>(* equal contribution) 39th International Conference on Machine Learning</em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://proceedings.mlr.press/v162/thaker22a/thaker22a.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p>Deep neural network-based classifiers have been shown to be vulnerable to imperceptible perturbations to their input, such as \ell_p-bounded norm adversarial attacks. This has motivated the development of many defense methods, which are then broken by new attacks, and so on. This paper focuses on a different but related problem of reverse engineering adversarial attacks. Specifically, given an attacked signal, we study conditions under which one can determine the type of attack (\ell_1, \ell_2 or \ell_∞) and recover the clean signal. We pose this problem as a block-sparse recovery problem, where both the signal and the attack are assumed to lie in a union of subspaces that includes one subspace per class and one subspace per attack type. We derive geometric conditions on the subspaces under which any attacked signal can be decomposed as the sum of a clean signal plus an attack. In addition, by determining the subspaces that contain the signal and the attack, we can also classify the signal and determine the attack type. Experiments on digit and face classification demonstrate the effectiveness of the proposed approach.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">pmlr-v162-thaker22a</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Reverse Engineering $\ell_p$ attacks: A block-sparse optimization approach with recovery guarantees}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Thaker *, Darshan and Giampouras *, Paris and Vidal, Rene}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{(* equal contribution) 39th International Conference on Machine Learning}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{21253--21271}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{162}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{Proceedings of Machine Learning Research}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{PMLR}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%70%61%72%69%73%67%69%61%6D%70%6F%75%72%61%73@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fas fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=parisgiampouras" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/parisgiampouras" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/parisgiampouras" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a> <a href="https://twitter.com/parisgiampouras" title="Twitter" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter"></i></a> </div> <div class="contact-note"> </div> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2026 Paris Giampouras. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. <a href="https://parisgiampouras.github.io/contact/">Impressum</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-WZ13PPY09N"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-WZ13PPY09N");</script> </body> </html>